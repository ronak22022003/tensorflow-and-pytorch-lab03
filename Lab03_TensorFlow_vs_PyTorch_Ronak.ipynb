{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/meets5800/EmbeddedSystems/blob/main/Lab03_TensorFlow_vs_PyTorch_MEET.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c1e84a1",
      "metadata": {
        "id": "5c1e84a1"
      },
      "source": [
        "# Lab 03: TensorFlow vs. PyTorch\n",
        "- Train a model on MNIST in both TensorFlow and PyTorch, convert to TFLite and ONNX.  \n",
        "- Use tf.GradientTape for Tensorflow custom training loop.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1c58bba",
      "metadata": {
        "id": "a1c58bba"
      },
      "source": [
        "## TensorFlow Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23ebc05e",
      "metadata": {
        "id": "23ebc05e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63aa5335-8f9c-4d8b-9058-ca9a0fcd05a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Epoch 1/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 5ms/step - accuracy: 0.8626 - loss: 0.4858\n",
            "Epoch 2/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9558 - loss: 0.1542\n",
            "Epoch 3/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9688 - loss: 0.1085\n",
            "Epoch 4/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9761 - loss: 0.0799\n",
            "Epoch 5/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9801 - loss: 0.0662\n",
            "TF Training time: 38.01 seconds\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9707 - loss: 0.1005\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.08727740496397018, 0.973800003528595]"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import time\n",
        "\n",
        "# Load and normalize the MNIST data\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train = x_train / 255.0   # Normalization factor for grayscale images\n",
        "x_test = x_test / 255.0\n",
        "\n",
        "# One-hot encode the labels\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "\n",
        "# Build the model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=(28, 28)),         # MNIST images are 28x28\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),  # 64 hidden neurons\n",
        "    tf.keras.layers.Dense(10, activation='softmax')  # 10 output classes for digits 0-9\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',      # Suitable for one-hot encoded labels\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model and measure time\n",
        "start = time.time()\n",
        "model.fit(x_train, y_train, epochs=5)\n",
        "end = time.time()\n",
        "\n",
        "print(f\"TF Training time: {end - start:.2f} seconds\")  # Output training time\n",
        "\n",
        "# Evaluate the model\n",
        "model.evaluate(x_test, y_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72743ab8",
      "metadata": {
        "id": "72743ab8"
      },
      "source": [
        "## Convert TensorFlow model to TFLite"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be6ab50a",
      "metadata": {
        "id": "be6ab50a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb60067a-50f2-4cdd-8a8e-13c3742f92d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved artifact at '/tmp/tmp8usjq8lh'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 28, 28), dtype=tf.float32, name='keras_tensor')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 10), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  137514791917456: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137514791920144: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137514791920336: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137514791920528: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
          ]
        }
      ],
      "source": [
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "with open(\"model.tflite\", \"wb\") as f:\n",
        "    f.write(tflite_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "57c00c95",
      "metadata": {
        "id": "57c00c95"
      },
      "source": [
        "## PyTorch Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "623dfb49",
      "metadata": {
        "id": "623dfb49",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17ca8a5d-6e1c-4879-c715-50851d8351a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 40.0MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 1.06MB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 9.65MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 4.91MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch Training time: 55.50 seconds\n",
            "Test accuracy: 0.9649\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import time\n",
        "\n",
        "# Transform: flatten the 28x28 images to 784-dimensional vectors\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Lambda(lambda x: x.view(-1))  # Flatten to 784\n",
        "])\n",
        "\n",
        "# Data loaders\n",
        "train_loader = DataLoader(\n",
        "    datasets.MNIST(root='./data', train=True, transform=transform, download=True),\n",
        "    batch_size=32\n",
        ")\n",
        "test_loader = DataLoader(\n",
        "    datasets.MNIST(root='./data', train=False, transform=transform, download=True),\n",
        "    batch_size=1000\n",
        ")\n",
        "\n",
        "# Define the network\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(784, 64)     # 784 input features to 64 hidden units\n",
        "        self.fc2 = nn.Linear(64, 10)      # 64 hidden units to 10 output classes\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))           # Activation after first layer\n",
        "        return self.fc2(x)                # Output logits from second layer\n",
        "\n",
        "model = Net()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# Training\n",
        "start = time.time()\n",
        "for epoch in range(5):\n",
        "    for x, y in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        pred = model(x)\n",
        "        loss = loss_fn(pred, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "end = time.time()\n",
        "print(f\"PyTorch Training time: {end - start:.2f} seconds\")\n",
        "\n",
        "# Evaluation\n",
        "model.eval()\n",
        "correct = 0\n",
        "with torch.no_grad():\n",
        "    for x, y in test_loader:\n",
        "        output = model(x)\n",
        "        pred = output.argmax(1)\n",
        "        correct += (pred == y).sum().item()\n",
        "print(f\"Test accuracy: {correct / len(test_loader.dataset):.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6dbdab0",
      "metadata": {
        "id": "f6dbdab0"
      },
      "source": [
        "## Convert PyTorch model to ONNX"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install ONNX\n",
        "!pip install onnx"
      ],
      "metadata": {
        "id": "WuMKMhHc8aLF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ae02de6-c87a-4643-c2de-f12348ea3b10"
      },
      "id": "WuMKMhHc8aLF",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting onnx\n",
            "  Downloading onnx-1.18.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.11/dist-packages (from onnx) (2.0.2)\n",
            "Requirement already satisfied: protobuf>=4.25.1 in /usr/local/lib/python3.11/dist-packages (from onnx) (5.29.5)\n",
            "Requirement already satisfied: typing_extensions>=4.7.1 in /usr/local/lib/python3.11/dist-packages (from onnx) (4.14.0)\n",
            "Downloading onnx-1.18.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m87.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: onnx\n",
            "Successfully installed onnx-1.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09925e9a",
      "metadata": {
        "id": "09925e9a"
      },
      "outputs": [],
      "source": [
        "dummy_input = torch.randn(1, 784)\n",
        "torch.onnx.export(model, dummy_input, \"model.onnx\",\n",
        "                  input_names=[\"input\"], output_names=[\"output\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TensorFlow custom training loop using tf.GradientTape"
      ],
      "metadata": {
        "id": "sv4P-dSS_GQB"
      },
      "id": "sv4P-dSS_GQB"
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import time\n",
        "\n",
        "# Load and preprocess data\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train = x_train / 255.0   # Normalize pixel values to [0, 1]\n",
        "x_test = x_test / 255.0\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "\n",
        "# Prepare datasets\n",
        "batch_size = 32         # Same batch size as in earlier TF/PyTorch examples\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(10000).batch(batch_size)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size)\n",
        "\n",
        "# Define model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=(28, 28)),          # Input shape for MNIST images\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),   # 64 neurons, ReLU activation\n",
        "    tf.keras.layers.Dense(10, activation='softmax') # 10 neurons, Softmax for classification\n",
        "])\n",
        "\n",
        "# Define loss, optimizer, and metrics\n",
        "loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "train_acc_metric = tf.keras.metrics.CategoricalAccuracy()\n",
        "test_acc_metric = tf.keras.metrics.CategoricalAccuracy()\n",
        "\n",
        "# Training loop\n",
        "epochs = 5\n",
        "start = time.time()\n",
        "for epoch in range(epochs):\n",
        "    print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
        "    for step, (x_batch, y_batch) in enumerate(train_dataset):\n",
        "        with tf.GradientTape() as tape:\n",
        "            logits = model(x_batch, training=True)\n",
        "            loss = loss_fn(y_batch, logits)\n",
        "        grads = tape.gradient(loss, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "        train_acc_metric.update_state(y_batch, logits)\n",
        "\n",
        "        if step % 100 == 0:\n",
        "            print(f\"Step {step}, Loss: {loss.numpy():.4f}, Accuracy: {train_acc_metric.result().numpy():.4f}\")\n",
        "\n",
        "    print(f\"Training Accuracy for epoch {epoch+1}: {train_acc_metric.result().numpy():.4f}\")\n",
        "    train_acc_metric.reset_state()\n",
        "end = time.time()\n",
        "print(f\"\\nTF Training time: {end - start:.2f} seconds\")\n",
        "\n",
        "# Evaluation loop\n",
        "for x_batch, y_batch in test_dataset:\n",
        "    test_logits = model(x_batch, training=False)\n",
        "    test_acc_metric.update_state(y_batch, test_logits)\n",
        "\n",
        "print(f\"Test Accuracy: {test_acc_metric.result().numpy():.4f}\")\n"
      ],
      "metadata": {
        "id": "KH-sDlHq_Gdw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3e7a5c7-c242-4adc-ced9-4ebe1b4c8361"
      },
      "id": "KH-sDlHq_Gdw",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/5\n",
            "Step 0, Loss: 2.5599, Accuracy: 0.0938\n",
            "Step 100, Loss: 0.8005, Accuracy: 0.7348\n",
            "Step 200, Loss: 0.2844, Accuracy: 0.7990\n",
            "Step 300, Loss: 0.3592, Accuracy: 0.8303\n",
            "Step 400, Loss: 0.4234, Accuracy: 0.8491\n",
            "Step 500, Loss: 0.1734, Accuracy: 0.8629\n",
            "Step 600, Loss: 0.1543, Accuracy: 0.8717\n",
            "Step 700, Loss: 0.3524, Accuracy: 0.8775\n",
            "Step 800, Loss: 0.1531, Accuracy: 0.8840\n",
            "Step 900, Loss: 0.4796, Accuracy: 0.8888\n",
            "Step 1000, Loss: 0.3706, Accuracy: 0.8924\n",
            "Step 1100, Loss: 0.2733, Accuracy: 0.8961\n",
            "Step 1200, Loss: 0.0857, Accuracy: 0.8991\n",
            "Step 1300, Loss: 0.3952, Accuracy: 0.9009\n",
            "Step 1400, Loss: 0.2163, Accuracy: 0.9038\n",
            "Step 1500, Loss: 0.1115, Accuracy: 0.9060\n",
            "Step 1600, Loss: 0.2847, Accuracy: 0.9085\n",
            "Step 1700, Loss: 0.2273, Accuracy: 0.9106\n",
            "Step 1800, Loss: 0.3915, Accuracy: 0.9127\n",
            "Training Accuracy for epoch 1: 0.9143\n",
            "\n",
            "Epoch 2/5\n",
            "Step 0, Loss: 0.0655, Accuracy: 0.9688\n",
            "Step 100, Loss: 0.0433, Accuracy: 0.9527\n",
            "Step 200, Loss: 0.3096, Accuracy: 0.9456\n",
            "Step 300, Loss: 0.0480, Accuracy: 0.9474\n",
            "Step 400, Loss: 0.1798, Accuracy: 0.9485\n",
            "Step 500, Loss: 0.0832, Accuracy: 0.9490\n",
            "Step 600, Loss: 0.1746, Accuracy: 0.9494\n",
            "Step 700, Loss: 0.2445, Accuracy: 0.9499\n",
            "Step 800, Loss: 0.2432, Accuracy: 0.9505\n",
            "Step 900, Loss: 0.3406, Accuracy: 0.9520\n",
            "Step 1000, Loss: 0.0875, Accuracy: 0.9527\n",
            "Step 1100, Loss: 0.2666, Accuracy: 0.9530\n",
            "Step 1200, Loss: 0.1096, Accuracy: 0.9531\n",
            "Step 1300, Loss: 0.0369, Accuracy: 0.9534\n",
            "Step 1400, Loss: 0.2015, Accuracy: 0.9537\n",
            "Step 1500, Loss: 0.1891, Accuracy: 0.9541\n",
            "Step 1600, Loss: 0.1790, Accuracy: 0.9545\n",
            "Step 1700, Loss: 0.2147, Accuracy: 0.9552\n",
            "Step 1800, Loss: 0.0333, Accuracy: 0.9558\n",
            "Training Accuracy for epoch 2: 0.9561\n",
            "\n",
            "Epoch 3/5\n",
            "Step 0, Loss: 0.0270, Accuracy: 1.0000\n",
            "Step 100, Loss: 0.0302, Accuracy: 0.9623\n",
            "Step 200, Loss: 0.1287, Accuracy: 0.9628\n",
            "Step 300, Loss: 0.1443, Accuracy: 0.9653\n",
            "Step 400, Loss: 0.0481, Accuracy: 0.9663\n",
            "Step 500, Loss: 0.0335, Accuracy: 0.9668\n",
            "Step 600, Loss: 0.1290, Accuracy: 0.9665\n",
            "Step 700, Loss: 0.1063, Accuracy: 0.9663\n",
            "Step 800, Loss: 0.2980, Accuracy: 0.9670\n",
            "Step 900, Loss: 0.0115, Accuracy: 0.9673\n",
            "Step 1000, Loss: 0.0263, Accuracy: 0.9675\n",
            "Step 1100, Loss: 0.0321, Accuracy: 0.9673\n",
            "Step 1200, Loss: 0.0583, Accuracy: 0.9673\n",
            "Step 1300, Loss: 0.1002, Accuracy: 0.9677\n",
            "Step 1400, Loss: 0.1595, Accuracy: 0.9677\n",
            "Step 1500, Loss: 0.1900, Accuracy: 0.9678\n",
            "Step 1600, Loss: 0.0355, Accuracy: 0.9682\n",
            "Step 1700, Loss: 0.0586, Accuracy: 0.9683\n",
            "Step 1800, Loss: 0.4989, Accuracy: 0.9684\n",
            "Training Accuracy for epoch 3: 0.9686\n",
            "\n",
            "Epoch 4/5\n",
            "Step 0, Loss: 0.2733, Accuracy: 0.8750\n",
            "Step 100, Loss: 0.0454, Accuracy: 0.9771\n",
            "Step 200, Loss: 0.0772, Accuracy: 0.9753\n",
            "Step 300, Loss: 0.0630, Accuracy: 0.9741\n",
            "Step 400, Loss: 0.0195, Accuracy: 0.9744\n",
            "Step 500, Loss: 0.0907, Accuracy: 0.9745\n",
            "Step 600, Loss: 0.2567, Accuracy: 0.9742\n",
            "Step 700, Loss: 0.0201, Accuracy: 0.9737\n",
            "Step 800, Loss: 0.0144, Accuracy: 0.9735\n",
            "Step 900, Loss: 0.1404, Accuracy: 0.9740\n",
            "Step 1000, Loss: 0.0864, Accuracy: 0.9741\n",
            "Step 1100, Loss: 0.0050, Accuracy: 0.9745\n",
            "Step 1200, Loss: 0.0561, Accuracy: 0.9741\n",
            "Step 1300, Loss: 0.0107, Accuracy: 0.9741\n",
            "Step 1400, Loss: 0.0223, Accuracy: 0.9738\n",
            "Step 1500, Loss: 0.0092, Accuracy: 0.9739\n",
            "Step 1600, Loss: 0.1209, Accuracy: 0.9742\n",
            "Step 1700, Loss: 0.1180, Accuracy: 0.9746\n",
            "Step 1800, Loss: 0.1040, Accuracy: 0.9747\n",
            "Training Accuracy for epoch 4: 0.9749\n",
            "\n",
            "Epoch 5/5\n",
            "Step 0, Loss: 0.0206, Accuracy: 1.0000\n",
            "Step 100, Loss: 0.0697, Accuracy: 0.9762\n",
            "Step 200, Loss: 0.0982, Accuracy: 0.9765\n",
            "Step 300, Loss: 0.0302, Accuracy: 0.9764\n",
            "Step 400, Loss: 0.0201, Accuracy: 0.9774\n",
            "Step 500, Loss: 0.0728, Accuracy: 0.9782\n",
            "Step 600, Loss: 0.0452, Accuracy: 0.9781\n",
            "Step 700, Loss: 0.0106, Accuracy: 0.9781\n",
            "Step 800, Loss: 0.0612, Accuracy: 0.9785\n",
            "Step 900, Loss: 0.0345, Accuracy: 0.9786\n",
            "Step 1000, Loss: 0.1704, Accuracy: 0.9787\n",
            "Step 1100, Loss: 0.1224, Accuracy: 0.9784\n",
            "Step 1200, Loss: 0.0326, Accuracy: 0.9789\n",
            "Step 1300, Loss: 0.1473, Accuracy: 0.9786\n",
            "Step 1400, Loss: 0.0505, Accuracy: 0.9785\n",
            "Step 1500, Loss: 0.0801, Accuracy: 0.9783\n",
            "Step 1600, Loss: 0.0086, Accuracy: 0.9787\n",
            "Step 1700, Loss: 0.0146, Accuracy: 0.9787\n",
            "Step 1800, Loss: 0.0834, Accuracy: 0.9792\n",
            "Training Accuracy for epoch 5: 0.9792\n",
            "\n",
            "TF Training time: 351.13 seconds\n",
            "Test Accuracy: 0.9718\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Performance Otimization with Graph Execution using @tf.function"
      ],
      "metadata": {
        "id": "E4Nlg4lb_qdW"
      },
      "id": "E4Nlg4lb_qdW"
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import time\n",
        "\n",
        "# Load and preprocess data\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train = x_train / 255.0   # Normalize pixel values to [0, 1]\n",
        "x_test = x_test / 255.0\n",
        "y_train = to_categorical(y_train, num_classes=10)\n",
        "y_test = to_categorical(y_test, num_classes=10)\n",
        "\n",
        "# Prepare datasets\n",
        "batch_size = 32\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(10000).batch(batch_size)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size)\n",
        "\n",
        "# Define model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=(28, 28)),         # MNIST images are 28x28\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),  # 64 hidden neurons\n",
        "    tf.keras.layers.Dense(10, activation='softmax')# 10 output classes with softmax\n",
        "])\n",
        "\n",
        "# Define loss, optimizer, and metrics\n",
        "loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "train_acc_metric = tf.keras.metrics.CategoricalAccuracy()\n",
        "test_acc_metric = tf.keras.metrics.CategoricalAccuracy()\n",
        "\n",
        "@tf.function  # compile the function into a graph for faster execution\n",
        "def train_step(x_batch, y_batch):\n",
        "    with tf.GradientTape() as tape:\n",
        "        logits = model(x_batch, training=True)\n",
        "        loss = loss_fn(y_batch, logits)\n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "    train_acc_metric.update_state(y_batch, logits)\n",
        "    return loss\n",
        "\n",
        "# Training loop\n",
        "epochs = 5\n",
        "start = time.time()\n",
        "for epoch in range(epochs):\n",
        "    print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
        "    for step, (x_batch, y_batch) in enumerate(train_dataset):\n",
        "        loss = train_step(x_batch, y_batch)\n",
        "\n",
        "        if step % 100 == 0:\n",
        "            print(f\"Step {step}, Loss: {loss.numpy():.4f}, Accuracy: {train_acc_metric.result().numpy():.4f}\")\n",
        "\n",
        "    print(f\"Training Accuracy for epoch {epoch+1}: {train_acc_metric.result().numpy():.4f}\")\n",
        "    train_acc_metric.reset_state()\n",
        "end = time.time()\n",
        "print(f\"\\nTF Training time: {end - start:.2f} seconds\")\n",
        "\n",
        "# Evaluation loop\n",
        "for x_batch, y_batch in test_dataset:\n",
        "    test_logits = model(x_batch, training=False)\n",
        "    test_acc_metric.update_state(y_batch, test_logits)\n",
        "\n",
        "print(f\"Test Accuracy: {test_acc_metric.result().numpy():.4f}\")\n"
      ],
      "metadata": {
        "id": "Jmu_hciK_qle",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a502207-928a-4c9f-b664-17ae05c12d35"
      },
      "id": "Jmu_hciK_qle",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/5\n",
            "Step 0, Loss: 2.3941, Accuracy: 0.0938\n",
            "Step 100, Loss: 0.3854, Accuracy: 0.7432\n",
            "Step 200, Loss: 0.4176, Accuracy: 0.8099\n",
            "Step 300, Loss: 0.2513, Accuracy: 0.8367\n",
            "Step 400, Loss: 0.4739, Accuracy: 0.8551\n",
            "Step 500, Loss: 0.3649, Accuracy: 0.8663\n",
            "Step 600, Loss: 0.1463, Accuracy: 0.8756\n",
            "Step 700, Loss: 0.5474, Accuracy: 0.8833\n",
            "Step 800, Loss: 0.2559, Accuracy: 0.8887\n",
            "Step 900, Loss: 0.1649, Accuracy: 0.8936\n",
            "Step 1000, Loss: 0.5758, Accuracy: 0.8974\n",
            "Step 1100, Loss: 0.2143, Accuracy: 0.9010\n",
            "Step 1200, Loss: 0.0831, Accuracy: 0.9039\n",
            "Step 1300, Loss: 0.1452, Accuracy: 0.9061\n",
            "Step 1400, Loss: 0.1728, Accuracy: 0.9080\n",
            "Step 1500, Loss: 0.1822, Accuracy: 0.9103\n",
            "Step 1600, Loss: 0.2649, Accuracy: 0.9130\n",
            "Step 1700, Loss: 0.0902, Accuracy: 0.9149\n",
            "Step 1800, Loss: 0.2435, Accuracy: 0.9169\n",
            "Training Accuracy for epoch 1: 0.9183\n",
            "\n",
            "Epoch 2/5\n",
            "Step 0, Loss: 0.0927, Accuracy: 1.0000\n",
            "Step 100, Loss: 0.0236, Accuracy: 0.9548\n",
            "Step 200, Loss: 0.2071, Accuracy: 0.9541\n",
            "Step 300, Loss: 0.0646, Accuracy: 0.9535\n",
            "Step 400, Loss: 0.1069, Accuracy: 0.9540\n",
            "Step 500, Loss: 0.1371, Accuracy: 0.9547\n",
            "Step 600, Loss: 0.2360, Accuracy: 0.9554\n",
            "Step 700, Loss: 0.1757, Accuracy: 0.9551\n",
            "Step 800, Loss: 0.0721, Accuracy: 0.9555\n",
            "Step 900, Loss: 0.6396, Accuracy: 0.9563\n",
            "Step 1000, Loss: 0.0572, Accuracy: 0.9560\n",
            "Step 1100, Loss: 0.0835, Accuracy: 0.9569\n",
            "Step 1200, Loss: 0.0656, Accuracy: 0.9569\n",
            "Step 1300, Loss: 0.1963, Accuracy: 0.9562\n",
            "Step 1400, Loss: 0.2480, Accuracy: 0.9561\n",
            "Step 1500, Loss: 0.2646, Accuracy: 0.9563\n",
            "Step 1600, Loss: 0.5393, Accuracy: 0.9569\n",
            "Step 1700, Loss: 0.3575, Accuracy: 0.9577\n",
            "Step 1800, Loss: 0.2416, Accuracy: 0.9579\n",
            "Training Accuracy for epoch 2: 0.9583\n",
            "\n",
            "Epoch 3/5\n",
            "Step 0, Loss: 0.2364, Accuracy: 0.9688\n",
            "Step 100, Loss: 0.1659, Accuracy: 0.9712\n",
            "Step 200, Loss: 0.0180, Accuracy: 0.9681\n",
            "Step 300, Loss: 0.1667, Accuracy: 0.9644\n",
            "Step 400, Loss: 0.0888, Accuracy: 0.9657\n",
            "Step 500, Loss: 0.0242, Accuracy: 0.9661\n",
            "Step 600, Loss: 0.1273, Accuracy: 0.9666\n",
            "Step 700, Loss: 0.0241, Accuracy: 0.9662\n",
            "Step 800, Loss: 0.0134, Accuracy: 0.9670\n",
            "Step 900, Loss: 0.1135, Accuracy: 0.9671\n",
            "Step 1000, Loss: 0.2283, Accuracy: 0.9672\n",
            "Step 1100, Loss: 0.0590, Accuracy: 0.9674\n",
            "Step 1200, Loss: 0.0219, Accuracy: 0.9674\n",
            "Step 1300, Loss: 0.1415, Accuracy: 0.9671\n",
            "Step 1400, Loss: 0.0501, Accuracy: 0.9674\n",
            "Step 1500, Loss: 0.1898, Accuracy: 0.9670\n",
            "Step 1600, Loss: 0.0849, Accuracy: 0.9674\n",
            "Step 1700, Loss: 0.0400, Accuracy: 0.9678\n",
            "Step 1800, Loss: 0.0929, Accuracy: 0.9679\n",
            "Training Accuracy for epoch 3: 0.9679\n",
            "\n",
            "Epoch 4/5\n",
            "Step 0, Loss: 0.0934, Accuracy: 0.9688\n",
            "Step 100, Loss: 0.0282, Accuracy: 0.9749\n",
            "Step 200, Loss: 0.0878, Accuracy: 0.9725\n",
            "Step 300, Loss: 0.0309, Accuracy: 0.9726\n",
            "Step 400, Loss: 0.0526, Accuracy: 0.9714\n",
            "Step 500, Loss: 0.0937, Accuracy: 0.9719\n",
            "Step 600, Loss: 0.0120, Accuracy: 0.9721\n",
            "Step 700, Loss: 0.1051, Accuracy: 0.9725\n",
            "Step 800, Loss: 0.0419, Accuracy: 0.9728\n",
            "Step 900, Loss: 0.0968, Accuracy: 0.9732\n",
            "Step 1000, Loss: 0.0331, Accuracy: 0.9736\n",
            "Step 1100, Loss: 0.0859, Accuracy: 0.9738\n",
            "Step 1200, Loss: 0.0634, Accuracy: 0.9741\n",
            "Step 1300, Loss: 0.0663, Accuracy: 0.9740\n",
            "Step 1400, Loss: 0.0222, Accuracy: 0.9739\n",
            "Step 1500, Loss: 0.0294, Accuracy: 0.9739\n",
            "Step 1600, Loss: 0.0401, Accuracy: 0.9739\n",
            "Step 1700, Loss: 0.0083, Accuracy: 0.9742\n",
            "Step 1800, Loss: 0.0118, Accuracy: 0.9743\n",
            "Training Accuracy for epoch 4: 0.9743\n",
            "\n",
            "Epoch 5/5\n",
            "Step 0, Loss: 0.0198, Accuracy: 1.0000\n",
            "Step 100, Loss: 0.1075, Accuracy: 0.9771\n",
            "Step 200, Loss: 0.1604, Accuracy: 0.9764\n",
            "Step 300, Loss: 0.0189, Accuracy: 0.9765\n",
            "Step 400, Loss: 0.0111, Accuracy: 0.9783\n",
            "Step 500, Loss: 0.0517, Accuracy: 0.9786\n",
            "Step 600, Loss: 0.0376, Accuracy: 0.9786\n",
            "Step 700, Loss: 0.1264, Accuracy: 0.9786\n",
            "Step 800, Loss: 0.0901, Accuracy: 0.9785\n",
            "Step 900, Loss: 0.0782, Accuracy: 0.9783\n",
            "Step 1000, Loss: 0.0118, Accuracy: 0.9779\n",
            "Step 1100, Loss: 0.0734, Accuracy: 0.9780\n",
            "Step 1200, Loss: 0.0152, Accuracy: 0.9779\n",
            "Step 1300, Loss: 0.1144, Accuracy: 0.9778\n",
            "Step 1400, Loss: 0.0222, Accuracy: 0.9779\n",
            "Step 1500, Loss: 0.1939, Accuracy: 0.9778\n",
            "Step 1600, Loss: 0.1331, Accuracy: 0.9777\n",
            "Step 1700, Loss: 0.0024, Accuracy: 0.9779\n",
            "Step 1800, Loss: 0.0900, Accuracy: 0.9780\n",
            "Training Accuracy for epoch 5: 0.9782\n",
            "\n",
            "TF Training time: 24.80 seconds\n",
            "Test Accuracy: 0.9700\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}